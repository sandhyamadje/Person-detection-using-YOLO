{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b98e8d2-0ec7-4a87-9139-208d9a9f52ff",
   "metadata": {},
   "source": [
    "## Collect, annotate, and evaluate person detection using YOLO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb1d1da-aabd-4212-9b1b-22adb1f2ea2c",
   "metadata": {},
   "source": [
    "##### Install & import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9a5126f-4fc1-443b-ad1e-27ed6427f703",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ultralytics\n",
    "#!pip3 install labelImg\n",
    "#!pip install torchmetrics\n",
    "#!pip install lxml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f7c3479-bc81-4865-b40e-507ff88ef05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "model = YOLO(\"yolov8n.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91fe6c83-6b2a-4869-9db4-669cd95a1c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import requests\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO \n",
    "import torch\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f9562b5-8dd8-447f-a7e1-6a4dd54f6195",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import labelImg\n",
    "except ImportError:\n",
    "    print(\"Please install LabelImg from https://github.com/tzutalin/labelImg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de213ef-4fe2-41af-acb2-43f21b9871d2",
   "metadata": {},
   "source": [
    "### 1. Collected data using API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e357d46-8480-4445-a4aa-f18655ce9ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded image 1\n",
      "Downloaded image 2\n",
      "Downloaded image 3\n",
      "Downloaded image 4\n",
      "Downloaded image 5\n",
      "Downloaded image 6\n",
      "Downloaded image 7\n",
      "Downloaded image 8\n",
      "Downloaded image 9\n",
      "Downloaded image 10\n",
      "Downloaded image 11\n",
      "Downloaded image 12\n",
      "Downloaded image 13\n",
      "Downloaded image 14\n",
      "Downloaded image 15\n",
      "Downloaded image 16\n",
      "Downloaded image 17\n",
      "Downloaded image 18\n",
      "Downloaded image 19\n",
      "Downloaded image 20\n",
      "Downloaded image 21\n",
      "Downloaded image 22\n",
      "Downloaded image 23\n",
      "Downloaded image 24\n",
      "Downloaded image 25\n",
      "Downloaded image 26\n",
      "Downloaded image 27\n",
      "Downloaded image 28\n",
      "Downloaded image 29\n",
      "Downloaded image 30\n",
      "Downloaded image 31\n",
      "Downloaded image 32\n",
      "Downloaded image 33\n",
      "Downloaded image 34\n",
      "Downloaded image 35\n",
      "Downloaded image 36\n",
      "Downloaded image 37\n",
      "Downloaded image 38\n",
      "Downloaded image 39\n",
      "Downloaded image 40\n",
      "Downloaded image 41\n",
      "Downloaded image 42\n",
      "Downloaded image 43\n",
      "Downloaded image 44\n",
      "Downloaded image 45\n",
      "Downloaded image 46\n",
      "Downloaded image 47\n",
      "Downloaded image 48\n",
      "Downloaded image 49\n",
      "Downloaded image 50\n",
      "Downloaded image 51\n",
      "Downloaded image 52\n",
      "Downloaded image 53\n",
      "Downloaded image 54\n",
      "Downloaded image 55\n",
      "Downloaded image 56\n",
      "Downloaded image 57\n",
      "Downloaded image 58\n",
      "Downloaded image 59\n",
      "Downloaded image 60\n",
      "Downloaded image 61\n",
      "Downloaded image 62\n",
      "Downloaded image 63\n",
      "Downloaded image 64\n",
      "Downloaded image 65\n",
      "Downloaded image 66\n",
      "Downloaded image 67\n",
      "Downloaded image 68\n",
      "Downloaded image 69\n",
      "Downloaded image 70\n",
      "Downloaded image 71\n",
      "Downloaded image 72\n",
      "Downloaded image 73\n",
      "Downloaded image 74\n",
      "Downloaded image 75\n",
      "Downloaded image 76\n",
      "Downloaded image 77\n",
      "Downloaded image 78\n",
      "Downloaded image 79\n",
      "Downloaded image 80\n",
      "Downloaded image 81\n",
      "Downloaded image 82\n",
      "Downloaded image 83\n",
      "Downloaded image 84\n",
      "Downloaded image 85\n",
      "Downloaded image 86\n",
      "Downloaded image 87\n",
      "Downloaded image 88\n",
      "Downloaded image 89\n",
      "Downloaded image 90\n",
      "Downloaded image 91\n",
      "Downloaded image 92\n",
      "Downloaded image 93\n",
      "Downloaded image 94\n",
      "Downloaded image 95\n",
      "Downloaded image 96\n",
      "Downloaded image 97\n",
      "Downloaded image 98\n",
      "Downloaded image 99\n",
      "Downloaded image 100\n",
      "Downloaded 100 images to dataset/images\n"
     ]
    }
   ],
   "source": [
    "def collect_images(query=\"people\", output_dir=\"dataset/images\", num_images=100):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    api_key = \"unwSPFuEVRy9xS7Rd46VoExpSeYLwUX0macOV5xOEwoH17Kw0sNyIGax\"  \n",
    "    url = \"https://api.pexels.com/v1/search\"\n",
    "    headers = {\"Authorization\": api_key}\n",
    "\n",
    "    per_page = 15\n",
    "    pages = (num_images // per_page) + 1\n",
    "\n",
    "    count = 0\n",
    "    for page in range(1, pages + 1):\n",
    "        params = {\"query\": query, \"per_page\": per_page, \"page\": page}\n",
    "        response = requests.get(url, headers=headers, params=params)\n",
    "        data = response.json()\n",
    "\n",
    "        for photo in data[\"photos\"]:\n",
    "            if count >= num_images:\n",
    "                break\n",
    "            img_url = photo[\"src\"][\"original\"]\n",
    "            img_data = requests.get(img_url).content\n",
    "            with open(os.path.join(output_dir, f\"image_{count + 1}.jpg\"), \"wb\") as f:\n",
    "                f.write(img_data)\n",
    "            count += 1\n",
    "            print(f\"Downloaded image {count}\")\n",
    "\n",
    "    print(f\"Downloaded {count} images to {output_dir}\")\n",
    "\n",
    "collect_images(query=\"people in diverse conditions\", output_dir=\"dataset/images\", num_images=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9beda2-8ca7-47d8-bc5b-f3da13253255",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88341069-d7b1-4501-be27-79e2fcfe5375",
   "metadata": {},
   "source": [
    "### 2. Annotate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6352cdf7-2bff-4c9f-89e2-669d503673b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotate the images using LabelImg and save annotations in YOLO format.\n",
      "Save your annotations in: dataset/labels\n"
     ]
    }
   ],
   "source": [
    "annotations_dir = \"dataset/labels\"\n",
    "os.makedirs(annotations_dir, exist_ok=True)\n",
    "\n",
    "print(\"Annotate the images using LabelImg and save annotations in YOLO format.\")\n",
    "print(f\"Save your annotations in: {annotations_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652a8717-b2b6-4a5c-8773-286a5dbd365a",
   "metadata": {},
   "source": [
    "###3.Converted .xml file into .txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95ba8418-879d-406b-8d44-83a974e156f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion complete!\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def convert_voc_to_yolo(xml_dir, output_dir, labels):\n",
    "    \"\"\"\n",
    "    Converts PASCAL VOC annotations (.xml) to YOLO format (.txt).\n",
    "\n",
    "    Args:\n",
    "        xml_dir (str): Path to the directory containing .xml files.\n",
    "        output_dir (str): Path to save YOLO .txt files.\n",
    "        labels (list): List of class names (in order).\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for xml_file in os.listdir(xml_dir):\n",
    "        if not xml_file.endswith('.xml'):\n",
    "            continue\n",
    "\n",
    "        xml_path = os.path.join(xml_dir, xml_file)\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        size = root.find('size')\n",
    "        width = int(size.find('width').text)\n",
    "        height = int(size.find('height').text)\n",
    "\n",
    "        yolo_annotations = []\n",
    "        for obj in root.findall('object'):\n",
    "            class_name = obj.find('name').text\n",
    "            if class_name not in labels:\n",
    "                continue\n",
    "            class_id = labels.index(class_name)\n",
    "\n",
    "            bbox = obj.find('bndbox')\n",
    "            xmin = float(bbox.find('xmin').text)\n",
    "            ymin = float(bbox.find('ymin').text)\n",
    "            xmax = float(bbox.find('xmax').text)\n",
    "            ymax = float(bbox.find('ymax').text)\n",
    "\n",
    "            # Convert to YOLO format\n",
    "            x_center = ((xmin + xmax) / 2) / width\n",
    "            y_center = ((ymin + ymax) / 2) / height\n",
    "            box_width = (xmax - xmin) / width\n",
    "            box_height = (ymax - ymin) / height\n",
    "\n",
    "            yolo_annotations.append(f\"{class_id} {x_center} {y_center} {box_width} {box_height}\")\n",
    "\n",
    "        txt_file = os.path.join(output_dir, os.path.splitext(xml_file)[0] + '.txt')\n",
    "        with open(txt_file, 'w') as f:\n",
    "            f.write('\\n'.join(yolo_annotations))\n",
    "\n",
    "xml_directory = \"dataset/labels_xml\"\n",
    "yolo_output_directory = \"dataset/labels\"\n",
    "class_labels = [\"person\"]  \n",
    "\n",
    "convert_voc_to_yolo(xml_dir= r\"C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\labels\", output_dir=yolo_output_directory, labels=class_labels)\n",
    "print(\"Conversion complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d2aaed-22ef-444a-8969-a179f3da24d3",
   "metadata": {},
   "source": [
    "### 4. Run YOLO model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac55817e-acd5-4f63-97b3-7b23fea81c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory created: C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\runs\n"
     ]
    }
   ],
   "source": [
    "runs_path = r\"C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\runs\"\n",
    "\n",
    "os.makedirs(runs_path, exist_ok=True)\n",
    "print(f\"Directory created: {runs_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3804786-3980-4c93-9755-f1b2cf11c2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_yolo_model(images_dir=\"dataset/images\", model_path=\"yolov8n.pt\", output_dir=\"runs/detect\"):\n",
    "    \"\"\"\n",
    "    Run a pre-trained YOLO model on the annotated images and handle empty predictions.\n",
    "    Saves results in the specified output directory.\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    model = YOLO(model_path)\n",
    "\n",
    "    image_files = [os.path.join(images_dir, f) for f in os.listdir(images_dir) if f.endswith('.jpg')]\n",
    "\n",
    "    if not image_files:\n",
    "        raise ValueError(\"No image files found in the specified directory.\")\n",
    "\n",
    "    try:\n",
    "        results = model.predict(\n",
    "            source=images_dir,  \n",
    "            save=True,         \n",
    "            save_txt=True,      \n",
    "            project=output_dir, \n",
    "            name=\"detect\"       )\n",
    "        print(f\"Results saved in: {os.path.join(output_dir, 'detect')}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during model prediction: {e}\")\n",
    "        raise\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afe408c-b236-441a-8ed7-9e2c6230bd49",
   "metadata": {},
   "source": [
    "### 5. Evaluate metrics using Torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f178835c-5c76-4469-9dfc-322f5c075f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(annotations_dir=\"dataset/labels\", predictions_dir=\"runs/detect/detect\"):\n",
    "    \"\"\"\n",
    "    Compute Precision, Recall, F1 Score, and mAP@50 using torchmetrics.\n",
    "    \"\"\"\n",
    "    ground_truths = []\n",
    "    predictions = []\n",
    "    for label_file in os.listdir(annotations_dir):\n",
    "        label_path = os.path.join(annotations_dir, label_file)\n",
    "        with open(label_path, 'r') as f:\n",
    "            bboxes = []\n",
    "            labels = []\n",
    "            for line in f.readlines():\n",
    "                parts = list(map(float, line.strip().split()))\n",
    "                labels.append(int(parts[0]))  # Class ID\n",
    "                bbox = [\n",
    "                    parts[1] - parts[3] / 2,  # x_min\n",
    "                    parts[2] - parts[4] / 2,  # y_min\n",
    "                    parts[1] + parts[3] / 2,  # x_max\n",
    "                    parts[2] + parts[4] / 2,  # y_max\n",
    "                ]\n",
    "                bboxes.append(bbox)\n",
    "            ground_truths.append({\"boxes\": torch.tensor(bboxes), \"labels\": torch.tensor(labels)})\n",
    "\n",
    "    for prediction_file in os.listdir(predictions_dir):\n",
    "        prediction_path = os.path.join(predictions_dir, prediction_file)\n",
    "        with open(prediction_path, 'r') as f:\n",
    "            bboxes = []\n",
    "            scores = []\n",
    "            labels = []\n",
    "            for line in f.readlines():\n",
    "                parts = list(map(float, line.strip().split()))\n",
    "                labels.append(int(parts[0]))  \n",
    "                scores.append(parts[5])       \n",
    "                bbox = [\n",
    "                    parts[1] - parts[3] / 2,  # x_min\n",
    "                    parts[2] - parts[4] / 2,  # y_min\n",
    "                    parts[1] + parts[3] / 2,  # x_max\n",
    "                    parts[2] + parts[4] / 2,  # y_max\n",
    "                ]\n",
    "                bboxes.append(bbox)\n",
    "            predictions.append({\"boxes\": torch.tensor(bboxes), \"scores\": torch.tensor(scores), \"labels\": torch.tensor(labels)})\n",
    "\n",
    "    while len(predictions) < len(ground_truths):\n",
    "        predictions.append({\"boxes\": torch.empty((0, 4)), \"scores\": torch.empty((0,)), \"labels\": torch.empty((0,), dtype=torch.long)})\n",
    "\n",
    "    if len(predictions) != len(ground_truths):\n",
    "        raise ValueError(\"Mismatch in number of ground truths and predictions!\")\n",
    "\n",
    "    metric = MeanAveragePrecision()\n",
    "    metric.update(predictions, ground_truths)\n",
    "\n",
    "    result = metric.compute()\n",
    "    print(\"Evaluation Results:\")\n",
    "    print(f\"Precision: {result['map']:.4f}\")\n",
    "    print(f\"Recall: {result['map50']:.4f}\")\n",
    "    print(f\"F1 Score: Calculated separately\")\n",
    "    print(f\"mAP@50: {result['map50']:.4f}\")\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd81508-8874-4456-aa37-1a087932a00f",
   "metadata": {},
   "source": [
    "### 6. Main script to define full pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd5f26da-ceeb-4fd2-98aa-b2c26fa8a9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "images_dir = \"dataset/images\"\n",
    "annotations_dir = \"dataset/labels\"\n",
    "model_path = \"yolov8n.pt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ebb416b-d88a-4bb2-9e28-8d4fb57d2b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded image 1\n",
      "Downloaded image 2\n",
      "Downloaded image 3\n",
      "Downloaded image 4\n",
      "Downloaded image 5\n",
      "Downloaded image 6\n",
      "Downloaded image 7\n",
      "Downloaded image 8\n",
      "Downloaded image 9\n",
      "Downloaded image 10\n",
      "Downloaded image 11\n",
      "Downloaded image 12\n",
      "Downloaded image 13\n",
      "Downloaded image 14\n",
      "Downloaded image 15\n",
      "Downloaded image 16\n",
      "Downloaded image 17\n",
      "Downloaded image 18\n",
      "Downloaded image 19\n",
      "Downloaded image 20\n",
      "Downloaded image 21\n",
      "Downloaded image 22\n",
      "Downloaded image 23\n",
      "Downloaded image 24\n",
      "Downloaded image 25\n",
      "Downloaded image 26\n",
      "Downloaded image 27\n",
      "Downloaded image 28\n",
      "Downloaded image 29\n",
      "Downloaded image 30\n",
      "Downloaded image 31\n",
      "Downloaded image 32\n",
      "Downloaded image 33\n",
      "Downloaded image 34\n",
      "Downloaded image 35\n",
      "Downloaded image 36\n",
      "Downloaded image 37\n",
      "Downloaded image 38\n",
      "Downloaded image 39\n",
      "Downloaded image 40\n",
      "Downloaded image 41\n",
      "Downloaded image 42\n",
      "Downloaded image 43\n",
      "Downloaded image 44\n",
      "Downloaded image 45\n",
      "Downloaded image 46\n",
      "Downloaded image 47\n",
      "Downloaded image 48\n",
      "Downloaded image 49\n",
      "Downloaded image 50\n",
      "Downloaded image 51\n",
      "Downloaded image 52\n",
      "Downloaded image 53\n",
      "Downloaded image 54\n",
      "Downloaded image 55\n",
      "Downloaded image 56\n",
      "Downloaded image 57\n",
      "Downloaded image 58\n",
      "Downloaded image 59\n",
      "Downloaded image 60\n",
      "Downloaded image 61\n",
      "Downloaded image 62\n",
      "Downloaded image 63\n",
      "Downloaded image 64\n",
      "Downloaded image 65\n",
      "Downloaded image 66\n",
      "Downloaded image 67\n",
      "Downloaded image 68\n",
      "Downloaded image 69\n",
      "Downloaded image 70\n",
      "Downloaded image 71\n",
      "Downloaded image 72\n",
      "Downloaded image 73\n",
      "Downloaded image 74\n",
      "Downloaded image 75\n",
      "Downloaded image 76\n",
      "Downloaded image 77\n",
      "Downloaded image 78\n",
      "Downloaded image 79\n",
      "Downloaded image 80\n",
      "Downloaded image 81\n",
      "Downloaded image 82\n",
      "Downloaded image 83\n",
      "Downloaded image 84\n",
      "Downloaded image 85\n",
      "Downloaded image 86\n",
      "Downloaded image 87\n",
      "Downloaded image 88\n",
      "Downloaded image 89\n",
      "Downloaded image 90\n",
      "Downloaded image 91\n",
      "Downloaded image 92\n",
      "Downloaded image 93\n",
      "Downloaded image 94\n",
      "Downloaded image 95\n",
      "Downloaded image 96\n",
      "Downloaded image 97\n",
      "Downloaded image 98\n",
      "Downloaded image 99\n",
      "Downloaded image 100\n",
      "Downloaded 100 images to dataset/images\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    collect_images(output_dir=images_dir, num_images=100)\n",
    "except Exception as e:\n",
    "    print(f\"Error in collecting images: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e4b4d574-0a87-44ab-991b-9c2ecf8f2ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 100. Processing in batches of 10.\n",
      "Processing batch 1: 10 images\n",
      "Processing images in directory: dataset/images\\temp_batch\n",
      "Processed image_1.jpg\n",
      "Processed image_10.jpg\n",
      "Processed image_100.jpg\n",
      "Processed image_11.jpg\n",
      "Processed image_12.jpg\n",
      "Processed image_13.jpg\n",
      "Processed image_14.jpg\n",
      "Processed image_15.jpg\n",
      "Processed image_16.jpg\n",
      "Processed image_17.jpg\n",
      "Completed batch 1/11\n",
      "Processing batch 2: 10 images\n",
      "Processing images in directory: dataset/images\\temp_batch\n",
      "Processed image_18.jpg\n",
      "Processed image_19.jpg\n",
      "Processed image_2.jpg\n",
      "Processed image_20.jpg\n",
      "Processed image_21.jpg\n",
      "Processed image_22.jpg\n",
      "Processed image_23.jpg\n",
      "Processed image_24.jpg\n",
      "Processed image_25.jpg\n",
      "Processed image_26.jpg\n",
      "Completed batch 2/11\n",
      "Processing batch 3: 10 images\n",
      "Processing images in directory: dataset/images\\temp_batch\n",
      "Processed image_27.jpg\n",
      "Processed image_28.jpg\n",
      "Processed image_29.jpg\n",
      "Processed image_3.jpg\n",
      "Processed image_30.jpg\n",
      "Processed image_31.jpg\n",
      "Processed image_32.jpg\n",
      "Processed image_33.jpg\n",
      "Processed image_34.jpg\n",
      "Processed image_35.jpg\n",
      "Completed batch 3/11\n",
      "Processing batch 4: 10 images\n",
      "Processing images in directory: dataset/images\\temp_batch\n",
      "Processed image_36.jpg\n",
      "Processed image_37.jpg\n",
      "Processed image_38.jpg\n",
      "Processed image_39.jpg\n",
      "Processed image_4.jpg\n",
      "Processed image_40.jpg\n",
      "Processed image_41.jpg\n",
      "Processed image_42.jpg\n",
      "Processed image_43.jpg\n",
      "Processed image_44.jpg\n",
      "Completed batch 4/11\n",
      "Processing batch 5: 10 images\n",
      "Processing images in directory: dataset/images\\temp_batch\n",
      "Processed image_45.jpg\n",
      "Processed image_46.jpg\n",
      "Processed image_47.jpg\n",
      "Processed image_48.jpg\n",
      "Processed image_49.jpg\n",
      "Processed image_5.jpg\n",
      "Processed image_50.jpg\n",
      "Processed image_51.jpg\n",
      "Processed image_52.jpg\n",
      "Processed image_53.jpg\n",
      "Completed batch 5/11\n",
      "Processing batch 6: 10 images\n",
      "Processing images in directory: dataset/images\\temp_batch\n",
      "Processed image_54.jpg\n",
      "Processed image_55.jpg\n",
      "Processed image_56.jpg\n",
      "Processed image_57.jpg\n",
      "Processed image_58.jpg\n",
      "Processed image_59.jpg\n",
      "Processed image_6.jpg\n",
      "Processed image_60.jpg\n",
      "Processed image_61.jpg\n",
      "Processed image_62.jpg\n",
      "Completed batch 6/11\n",
      "Processing batch 7: 10 images\n",
      "Processing images in directory: dataset/images\\temp_batch\n",
      "Processed image_63.jpg\n",
      "Processed image_64.jpg\n",
      "Processed image_65.jpg\n",
      "Processed image_66.jpg\n",
      "Processed image_67.jpg\n",
      "Processed image_68.jpg\n",
      "Processed image_69.jpg\n",
      "Processed image_7.jpg\n",
      "Processed image_70.jpg\n",
      "Processed image_71.jpg\n",
      "Completed batch 7/11\n",
      "Processing batch 8: 10 images\n",
      "Processing images in directory: dataset/images\\temp_batch\n",
      "Processed image_72.jpg\n",
      "Processed image_73.jpg\n",
      "Processed image_74.jpg\n",
      "Processed image_75.jpg\n",
      "Processed image_76.jpg\n",
      "Processed image_77.jpg\n",
      "Processed image_78.jpg\n",
      "Processed image_79.jpg\n",
      "Processed image_8.jpg\n",
      "Processed image_80.jpg\n",
      "Completed batch 8/11\n",
      "Processing batch 9: 10 images\n",
      "Processing images in directory: dataset/images\\temp_batch\n",
      "Processed image_81.jpg\n",
      "Processed image_82.jpg\n",
      "Processed image_83.jpg\n",
      "Processed image_84.jpg\n",
      "Processed image_85.jpg\n",
      "Processed image_86.jpg\n",
      "Processed image_87.jpg\n",
      "Processed image_88.jpg\n",
      "Processed image_89.jpg\n",
      "Processed image_9.jpg\n",
      "Completed batch 9/11\n",
      "Processing batch 10: 10 images\n",
      "Processing images in directory: dataset/images\\temp_batch\n",
      "Processed image_90.jpg\n",
      "Processed image_91.jpg\n",
      "Processed image_92.jpg\n",
      "Processed image_93.jpg\n",
      "Processed image_94.jpg\n",
      "Processed image_95.jpg\n",
      "Processed image_96.jpg\n",
      "Processed image_97.jpg\n",
      "Processed image_98.jpg\n",
      "Processed image_99.jpg\n",
      "Completed batch 10/11\n",
      "Batch processing completed.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "def batch_process_images(input_dir, batch_size, process_function, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Process images in smaller batches to avoid memory overload.\n",
    "    \n",
    "    Args:\n",
    "        input_dir (str): Path to the directory containing images.\n",
    "        batch_size (int): Number of images to process in each batch.\n",
    "        process_function (callable): Function to process the batch of images.\n",
    "        *args: Additional positional arguments for the process function.\n",
    "        **kwargs: Additional keyword arguments for the process function.\n",
    "    \"\"\"\n",
    "    image_files = [f for f in os.listdir(input_dir) if f.endswith((\".jpg\", \".png\"))]\n",
    "    total_images = len(image_files)\n",
    "    \n",
    "    print(f\"Total images: {total_images}. Processing in batches of {batch_size}.\")\n",
    "    \n",
    "    for i in range(0, total_images, batch_size):\n",
    "        batch = image_files[i:i + batch_size]\n",
    "        print(f\"Processing batch {i // batch_size + 1}: {len(batch)} images\")\n",
    "  \n",
    "        temp_batch_dir = os.path.join(input_dir, \"temp_batch\")\n",
    "        os.makedirs(temp_batch_dir, exist_ok=True)\n",
    " \n",
    "        for img_file in batch:\n",
    "            shutil.copy(os.path.join(input_dir, img_file), os.path.join(temp_batch_dir, img_file))\n",
    "\n",
    "        try:\n",
    "            process_function(temp_batch_dir, *args, **kwargs)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch {i // batch_size + 1}: {e}\")\n",
    "\n",
    "        shutil.rmtree(temp_batch_dir)\n",
    "        print(f\"Completed batch {i // batch_size + 1}/{(total_images // batch_size) + 1}\")\n",
    "    \n",
    "    print(\"Batch processing completed.\")\n",
    "\n",
    "def dummy_process_function(temp_dir):\n",
    "    \"\"\"\n",
    "    Dummy function to simulate processing of images in a batch.\n",
    "    Replace this with your actual image processing function, like YOLO detection.\n",
    "    \"\"\"\n",
    "    print(f\"Processing images in directory: {temp_dir}\")\n",
    "    for img_file in os.listdir(temp_dir):\n",
    "        print(f\"Processed {img_file}\")\n",
    "\n",
    "batch_process_images(\"dataset/images\", batch_size=10, process_function=dummy_process_function)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dd6559-9622-48b9-b209-344ccc37de9d",
   "metadata": {},
   "source": [
    "### 7. Used batch process because of overloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b817f5d-65c4-4664-aacc-3a1ec6dc1871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 100. Processing in batches of 10.\n",
      "Processing batch 1: 10 images\n",
      "\n",
      "image 1/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_1.jpg: 448x640 1 person, 451.5ms\n",
      "image 2/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_10.jpg: 384x640 9 persons, 560.4ms\n",
      "image 3/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_100.jpg: 416x640 4 persons, 1 sports ball, 1 chair, 1 laptop, 357.2ms\n",
      "image 4/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_11.jpg: 448x640 1 apple, 2 oranges, 1 vase, 723.0ms\n",
      "image 5/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_12.jpg: 448x640 11 persons, 1 backpack, 1 umbrella, 478.1ms\n",
      "image 6/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_13.jpg: 448x640 1 person, 274.7ms\n",
      "image 7/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_14.jpg: 384x640 1 person, 344.0ms\n",
      "image 8/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_15.jpg: 384x640 1 person, 191.9ms\n",
      "image 9/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_16.jpg: 480x640 1 person, 2 backpacks, 1 cell phone, 957.9ms\n",
      "image 10/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_17.jpg: 448x640 1 car, 339.9ms\n",
      "Speed: 123.6ms preprocess, 467.9ms inference, 31.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\detect\u001b[0m\n",
      "10 labels saved to runs\\detect\\detect\\labels\n",
      "Results saved in: runs/detect\\detect\n",
      "Completed batch 1/11\n",
      "Processing batch 2: 10 images\n",
      "\n",
      "image 1/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_18.jpg: 288x640 9 persons, 219.3ms\n",
      "image 2/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_19.jpg: 448x640 1 person, 1 sports ball, 315.3ms\n",
      "image 3/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_2.jpg: 448x640 1 person, 789.0ms\n",
      "image 4/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_20.jpg: 640x448 2 persons, 589.0ms\n",
      "image 5/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_21.jpg: 640x448 1 person, 1 banana, 402.2ms\n",
      "image 6/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_22.jpg: 448x640 1 person, 482.2ms\n",
      "image 7/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_23.jpg: 448x640 1 person, 351.9ms\n",
      "image 8/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_24.jpg: 640x448 1 person, 2 cups, 2 chairs, 1 dining table, 2 laptops, 3 mouses, 1 keyboard, 276.8ms\n",
      "image 9/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_25.jpg: 448x640 18 persons, 401.4ms\n",
      "image 10/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_26.jpg: 448x640 11 persons, 179.0ms\n",
      "Speed: 72.4ms preprocess, 400.6ms inference, 30.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\detect2\u001b[0m\n",
      "10 labels saved to runs\\detect\\detect2\\labels\n",
      "Results saved in: runs/detect\\detect\n",
      "Completed batch 2/11\n",
      "Processing batch 3: 10 images\n",
      "\n",
      "image 1/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_27.jpg: 448x640 13 persons, 2 cell phones, 334.7ms\n",
      "image 2/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_28.jpg: 512x640 1 person, 403.2ms\n",
      "image 3/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_29.jpg: 448x640 14 persons, 1 umbrella, 2 handbags, 405.4ms\n",
      "image 4/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_3.jpg: 448x640 1 person, 1295.7ms\n",
      "image 5/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_30.jpg: 448x640 1 person, 1775.3ms\n",
      "image 6/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_31.jpg: 448x640 1 person, 340.8ms\n",
      "image 7/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_32.jpg: 640x448 (no detections), 398.3ms\n",
      "Error during model prediction: OpenCV(4.8.1) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\core\\src\\alloc.cpp:73: error: (-4:Insufficient memory) Failed to allocate 72000000 bytes in function 'cv::OutOfMemoryError'\n",
      "\n",
      "Error processing batch 3: OpenCV(4.8.1) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\core\\src\\alloc.cpp:73: error: (-4:Insufficient memory) Failed to allocate 72000000 bytes in function 'cv::OutOfMemoryError'\n",
      "\n",
      "Completed batch 3/11\n",
      "Processing batch 4: 10 images\n",
      "\n",
      "image 1/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_36.jpg: 640x640 1 person, 1 elephant, 424.5ms\n",
      "image 2/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_37.jpg: 448x640 1 person, 468.9ms\n",
      "image 3/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_38.jpg: 448x640 1 person, 642.5ms\n",
      "image 4/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_39.jpg: 448x640 1 person, 1 handbag, 543.6ms\n",
      "image 5/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_4.jpg: 448x640 7 persons, 277.6ms\n",
      "image 6/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_40.jpg: 448x640 1 person, 392.6ms\n",
      "image 7/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_41.jpg: 352x640 1 person, 273.8ms\n",
      "image 8/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_42.jpg: 448x640 1 motorcycle, 1 bed, 248.8ms\n",
      "image 9/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_43.jpg: 448x640 7 persons, 1 tv, 423.3ms\n",
      "image 10/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_44.jpg: 640x640 1 cup, 697.3ms\n",
      "Speed: 74.4ms preprocess, 439.3ms inference, 26.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\detect4\u001b[0m\n",
      "10 labels saved to runs\\detect\\detect4\\labels\n",
      "Results saved in: runs/detect\\detect\n",
      "Completed batch 4/11\n",
      "Processing batch 5: 10 images\n",
      "\n",
      "image 1/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_45.jpg: 448x640 1 person, 194.2ms\n",
      "image 2/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_46.jpg: 416x640 1 person, 184.5ms\n",
      "image 3/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_47.jpg: 448x640 1 person, 998.4ms\n",
      "image 4/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_48.jpg: 448x640 1 person, 2118.4ms\n",
      "image 5/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_49.jpg: 448x640 1 person, 454.7ms\n",
      "image 6/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_5.jpg: 448x640 3 persons, 223.8ms\n",
      "image 7/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_50.jpg: 448x640 1 person, 1 tie, 182.8ms\n",
      "image 8/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_51.jpg: 192x640 2 persons, 2 laptops, 367.5ms\n",
      "image 9/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_52.jpg: 448x640 1 person, 1 cup, 1 chair, 1 potted plant, 1 laptop, 281.5ms\n",
      "image 10/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_53.jpg: 352x640 1 kite, 499.8ms\n",
      "Speed: 71.3ms preprocess, 550.6ms inference, 36.8ms postprocess per image at shape (1, 3, 352, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\detect5\u001b[0m\n",
      "10 labels saved to runs\\detect\\detect5\\labels\n",
      "Results saved in: runs/detect\\detect\n",
      "Completed batch 5/11\n",
      "Processing batch 6: 10 images\n",
      "\n",
      "image 1/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_54.jpg: 448x640 2 persons, 1 bench, 409.2ms\n",
      "image 2/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_55.jpg: 448x640 4 persons, 1294.0ms\n",
      "image 3/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_56.jpg: 640x448 (no detections), 571.3ms\n",
      "image 4/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_57.jpg: 448x640 11 persons, 1024.8ms\n",
      "image 5/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_58.jpg: 448x640 1 person, 352.5ms\n",
      "image 6/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_59.jpg: 448x640 2 persons, 345.4ms\n",
      "image 7/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_6.jpg: 448x640 1 person, 1 couch, 362.8ms\n",
      "image 8/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_60.jpg: 448x640 1 person, 1 bird, 261.6ms\n",
      "image 9/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_61.jpg: 448x640 1 person, 169.1ms\n",
      "image 10/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_62.jpg: 640x448 3 persons, 175.5ms\n",
      "Speed: 79.2ms preprocess, 496.6ms inference, 39.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Results saved to \u001b[1mruns\\detect\\detect6\u001b[0m\n",
      "9 labels saved to runs\\detect\\detect6\\labels\n",
      "Results saved in: runs/detect\\detect\n",
      "Completed batch 6/11\n",
      "Processing batch 7: 10 images\n",
      "\n",
      "image 1/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_63.jpg: 640x640 5 persons, 375.8ms\n",
      "image 2/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_64.jpg: 448x640 1 person, 431.7ms\n",
      "image 3/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_65.jpg: 448x640 1 person, 636.1ms\n",
      "image 4/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_66.jpg: 448x640 1 person, 1 cup, 1 toothbrush, 613.5ms\n",
      "image 5/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_67.jpg: 448x640 4 persons, 574.2ms\n",
      "WARNING  Image Read Error C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_68.jpg\n",
      "WARNING  Image Read Error C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_69.jpg\n",
      "Error during model prediction: Unable to allocate 34.9 MiB for an array with shape (4032, 3024, 3) and data type uint8\n",
      "Error processing batch 7: Unable to allocate 34.9 MiB for an array with shape (4032, 3024, 3) and data type uint8\n",
      "Completed batch 7/11\n",
      "Processing batch 8: 10 images\n",
      "\n",
      "image 1/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_72.jpg: 320x640 14 persons, 196.6ms\n",
      "image 2/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_73.jpg: 640x448 1 person, 324.6ms\n",
      "image 3/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_74.jpg: 448x640 1 person, 1 skateboard, 955.8ms\n",
      "image 4/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_75.jpg: 448x640 1 person, 418.2ms\n",
      "image 5/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_76.jpg: 448x640 1 person, 2 cars, 1 cell phone, 201.4ms\n",
      "image 6/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_77.jpg: 384x640 3 persons, 2 skateboards, 354.9ms\n",
      "image 7/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_78.jpg: 448x640 1 person, 1 toilet, 422.7ms\n",
      "WARNING  Image Read Error C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_79.jpg\n",
      "Error during model prediction: OpenCV(4.8.1) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\core\\src\\alloc.cpp:73: error: (-4:Insufficient memory) Failed to allocate 92716440 bytes in function 'cv::OutOfMemoryError'\n",
      "\n",
      "Error processing batch 8: OpenCV(4.8.1) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\core\\src\\alloc.cpp:73: error: (-4:Insufficient memory) Failed to allocate 92716440 bytes in function 'cv::OutOfMemoryError'\n",
      "\n",
      "Completed batch 8/11\n",
      "Processing batch 9: 10 images\n",
      "\n",
      "image 1/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_81.jpg: 448x640 1 person, 382.9ms\n",
      "image 2/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_82.jpg: 640x448 8 persons, 1 handbag, 243.9ms\n",
      "image 3/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_83.jpg: 640x512 1 person, 289.2ms\n",
      "WARNING  Image Read Error C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_84.jpg\n",
      "image 5/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_85.jpg: 640x448 1 person, 1811.3ms\n",
      "image 6/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_86.jpg: 448x640 2 persons, 232.0ms\n",
      "image 7/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_87.jpg: 640x448 1 person, 394.2ms\n",
      "image 8/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_88.jpg: 448x640 1 person, 205.0ms\n",
      "image 9/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_89.jpg: 640x448 1 person, 291.8ms\n",
      "WARNING  Image Read Error C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_9.jpg\n",
      "Error during model prediction: need at least one array to stack\n",
      "Error processing batch 9: need at least one array to stack\n",
      "Completed batch 9/11\n",
      "Processing batch 10: 10 images\n",
      "\n",
      "image 1/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_90.jpg: 448x640 2 persons, 176.4ms\n",
      "image 2/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_91.jpg: 512x640 5 persons, 1 bottle, 1 wine glass, 1 pizza, 1 dining table, 541.4ms\n",
      "image 3/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_92.jpg: 448x640 15 persons, 1 umbrella, 2 handbags, 1587.2ms\n",
      "image 4/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_93.jpg: 640x448 1 person, 412.0ms\n",
      "image 5/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_94.jpg: 448x640 1 person, 740.8ms\n",
      "image 6/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_95.jpg: 640x448 13 persons, 180.9ms\n",
      "WARNING  Image Read Error C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_96.jpg\n",
      "WARNING  Image Read Error C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_97.jpg\n",
      "image 9/10 C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_98.jpg: 640x448 (no detections), 736.4ms\n",
      "WARNING  Image Read Error C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\images\\temp_batch\\image_99.jpg\n",
      "Error during model prediction: need at least one array to stack\n",
      "Error processing batch 10: need at least one array to stack\n",
      "Completed batch 10/11\n",
      "Batch processing completed.\n"
     ]
    }
   ],
   "source": [
    "def yolo_process_batch(batch_dir, model_path):\n",
    "    \"\"\"\n",
    "    Process a batch of images using YOLO.\n",
    "    \n",
    "    Args:\n",
    "        batch_dir (str): Path to the batch directory containing images.\n",
    "        model_path (str): Path to the YOLO model file.\n",
    "    \"\"\"\n",
    "    run_yolo_model(images_dir=batch_dir, model_path=model_path)\n",
    "\n",
    "batch_process_images(\"dataset/images\", batch_size=10, process_function=yolo_process_batch, model_path=\"yolov8n.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ca32d2-d106-4684-8326-d638dbcd60bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a9a4155e-7817-4a89-80af-629d91204f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directories verified. Proceeding with evaluation.\n"
     ]
    }
   ],
   "source": [
    "annotations_dir = r\"C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\dataset\\labels\"\n",
    "predictions_dir = r\"C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\runs\\detect\\detect\"\n",
    "\n",
    "# Ensure directories exist\n",
    "if not os.path.exists(annotations_dir):\n",
    "    raise FileNotFoundError(f\"Annotations directory not found: {annotations_dir}\")\n",
    "if not os.path.exists(predictions_dir):\n",
    "    raise FileNotFoundError(f\"Predictions directory not found: {predictions_dir}\")\n",
    "\n",
    "print(\"Directories verified. Proceeding with evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "71e0f4e9-e065-4cb6-93f0-3832c20dff6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in evaluating results: [Errno 13] Permission denied: 'C:\\\\Users\\\\sandh.SHRIHARI\\\\OneDrive\\\\Desktop\\\\Data_science\\\\YOLO\\\\runs\\\\detect\\\\detect'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    evaluation_results = evaluate_metrics(annotations_dir=annotations_dir, predictions_dir=r\"C:\\Users\\sandh.SHRIHARI\\OneDrive\\Desktop\\Data_science\\YOLO\\runs\\detect\")\n",
    "    print(evaluation_results)\n",
    "except Exception as e:\n",
    "    print(f\"Error in evaluating results: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef60d54-eda0-43a7-a655-846c072efa2a",
   "metadata": {},
   "source": [
    "##### Tried to grant permission ...but it either completely shutting down the windows or if i choose to move this file then also getting permission denied error , i've tried to recreate the folder from scratch but coz of the yolo model windows not working properly ...It's totally getting hang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25bba37-553d-4562-be0e-a1aa18b4bb02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
